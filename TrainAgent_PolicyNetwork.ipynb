{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from importlib import reload\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "# get the relative path\n",
    "pref                = os.getcwd()\n",
    "\n",
    "os.environ[\"RL_PATH\"]   = pref\n",
    "\n",
    "#os.environ[\"RL_PATH\"] = \"/Users/ankitgupta/Documents/git/anks/Books/ReinforcementLearning/DeepQLearning\"\n",
    "#pref = os.environ[\"RL_PATH\"]\n",
    "\n",
    "if f'{pref}/RLLibrary' not in sys.path:\n",
    "    sys.path.append(f'{pref}/RLLibrary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger PG_Train (INFO)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# importing custom libraries\n",
    "from ActionSelection import ActionExploration\n",
    "from ConfigReader import Config\n",
    "#from RLAgents import QLearningAgent, FittedQAgent, DQN, DoubleDQN\n",
    "import PolicyGradientAgents\n",
    "import utils as RLUtils\n",
    "from Agents.PolicyGradient import Reinforce, ActorCritic\n",
    "from Agents.PolicyGradient import discountRewards\n",
    "import loggingConfig as loggingConfig\n",
    "\n",
    "\n",
    "logger = loggingConfig.logging\n",
    "logger.getLogger(\"PG_Train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Agents.PolicyGradient.Reinforce' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/Agents/PolicyGradient/Reinforce.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile  = os.path.join(pref, \"Configs.ini\" )\n",
    "savePath    = os.path.join(os.environ[\"RL_PATH\"], \"models\" )\n",
    "_time       = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParams = {\"Name\": \"Policy\", \"NetworkShape\": [24, 12], \"learning_rate\": 0.01, \\\n",
    "                \"optimizer\": \"ADAM\", \"loss\": \"categorical_crossentropy\", }\n",
    "valueParams = {\"Name_value\": \"Value\", \"NetworkShape_value\": [24, 12], \"learning_rate_value\": 0.0001, \\\n",
    "                \"optimizer_value\": \"ADAM\", \"loss_value\": \"mse\", }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Agents.PolicyGradient.ActorCritic' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/Agents/PolicyGradient/ActorCritic.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ActorCritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReinforceAgent   = ActorCritic.ActorCritic(env = env, configFile = configFile, **modelParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x64601d860>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReinforceAgent.SharedNetwork.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SharedDesign\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 24)           120         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 12)           300         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2)            26          dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            13          dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 459\n",
      "Trainable params: 459\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ReinforceAgent.SharedNetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets run for 1 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1ea89725ef29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactionProb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_currentState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactionProb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_currentState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# perform the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/Agents/PolicyGradient/ActorCritic.py\u001b[0m in \u001b[0;36mgetAction\u001b[0;34m(self, state, mode)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# the model prediction predicts the prob space for all actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mactionProb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSharedNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mactionProb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactionProb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "Agent = ReinforceAgent\n",
    "loss = []\n",
    "mode = \"TRAIN\"\n",
    "for _thisepisode in tqdm(range(Agent.NbEpisodesTrain)):\n",
    "\n",
    "    # reset the environment\n",
    "    _currentState = env.reset()\n",
    "    \n",
    "\n",
    "    _episodicReward = 0\n",
    "    _dead = False\n",
    "    _thisstepsTaken = 0\n",
    "\n",
    "\n",
    "    while _thisstepsTaken <= Agent.MaxSteps:\n",
    "\n",
    "        _starttime = time.perf_counter()\n",
    "\n",
    "        # get the action from agent\n",
    "        if \"REINFORCE\" in Agent.Name :\n",
    "            # only actor based methods. Network doesnt return critic values\n",
    "            action, actionProb = Agent.getAction(_currentState, mode = mode)\n",
    "        else:\n",
    "            action, actionProb, value = Agent.getAction(_currentState, mode = mode)            \n",
    "\n",
    "        # perform the action\n",
    "        _nextState, _reward, _dead, _info = env.step(action)\n",
    "\n",
    "        # record into memory\n",
    "        if \"REINFORCE\" in Agent.Name :\n",
    "\n",
    "            Agent.updateMemory(_currentState, action, _reward, _nextState, _dead, actionProb)\n",
    "        else:\n",
    "            Agent.updateMemory(_currentState, action, _reward, _nextState, _dead, actionProb, value)\n",
    "\n",
    "                \n",
    "        # update States\n",
    "        _currentState = _nextState\n",
    "        _thisstepsTaken += 1\n",
    "\n",
    "        _episodicReward += _reward\n",
    "\n",
    "        # if game over, then exit the loop\n",
    "        if _dead == True:\n",
    "\n",
    "            #if (_thisepisode+1)%update_frequency == 0:\n",
    "            Agent.train()\n",
    "\n",
    "            # ---- For logging ------\n",
    "            # In case of Neural networks, create tensorboard flow\n",
    "            Agent.updateLoggerInfo(episodeCount = _thisepisode, episodicReward = _episodicReward, \\\n",
    "                                    episodicStepsTaken = _thisstepsTaken, mode = \"TRAIN\")\n",
    "\n",
    "            _endtime = time.perf_counter()\n",
    "\n",
    "            logger.info(f'Episode: {_thisepisode+1} Steps: {_thisstepsTaken} Reward:{_episodicReward} Time taken: {round(_endtime - _starttime,2)} secs ')\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0050496 , -0.02645206,  0.01865667, -0.008793  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0050496 , -0.02645206,  0.01865667, -0.008793  ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_currentState = _currentState.reshape([1, _currentState.shape[0]])\n",
    "_currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "act, val = Agent.SharedNetwork(_currentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "act /= tf.reduce_sum(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00070665]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1131, shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action             = np.random.choice(Agent.env.action_space.n, p = act.numpy()[0])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-61fb5aae8d2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for all experience in batchsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcurStates\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mactions\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnextStates\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrewards\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# for all experience in batchsize\n",
    "curStates       = np.vstack(list(list(zip(*Agent.memory)))[0])\n",
    "actions         = np.vstack(list(list(zip(*Agent.memory)))[1])\n",
    "nextStates      = np.vstack(list(list(zip(*Agent.memory)))[3])\n",
    "rewards         = np.vstack(list(list(zip(*Agent.memory)))[2])\n",
    "done            = np.vstack(list(list(zip(*Agent.memory)))[4])\n",
    "\n",
    "actionProb      = np.vstack(list(list(zip(*Agent.memory)))[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the discounted rewards for the entire episode and normalize it\n",
    "discounted_rewards = discountRewards(rewards,  discountfactor=Agent.discountfactor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounted_rewards = (discounted_rewards - np.mean(discounted_rewards))/ (np.std(discounted_rewards) + 1e-7)            # to avoid division by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905667496168143"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_rewards[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    for index, sample in enumerate(Agent.memory):\n",
    "\n",
    "        # get log prob\n",
    "        state = tf.Variable([sample[0]], trainable=True, dtype=tf.float64)\n",
    "        prob = Agent.PolicyNetwork.model(state, training = True)\n",
    "        action = sample[1]\n",
    "        actionProb = prob[0, action]\n",
    "        logProb = tf.math.log(actionProb)\n",
    "\n",
    "        #actionProb = sample[5]\n",
    "        #logProb = np.log(actionProb)\n",
    "        sampleloss = logProb * discounted_rewards[index][0]\n",
    "\n",
    "        losses.append(-sampleloss)      # this is negative, since we are interested in gradient ascent\n",
    "\n",
    "    networkLoss = sum(losses)\n",
    "    \n",
    "    grads = tape.gradient(networkLoss, Agent.PolicyNetwork.model.trainable_variables)\n",
    "    Agent.PolicyNetwork.optimizer.apply_gradients(zip(grads, Agent.PolicyNetwork.model.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.12274747, -1.90964119,  0.19205285,  2.89212446]),\n",
       " 1,\n",
       " 1.0,\n",
       " array([-0.16094029, -1.7161645 ,  0.24989534,  2.66336226]),\n",
       " True,\n",
       " array([0.7354947 , 0.26450533], dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5343, shape=(1, 2), dtype=float32, numpy=array([[0.7354947, 0.2645053]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5352, shape=(), dtype=float32, numpy=0.2645053>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = sample[1]\n",
    "actionProb = prob[0, action]\n",
    "actionProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5344, shape=(1, 2), dtype=float32, numpy=array([[-0.307212, -1.329894]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.dtype' object has no attribute 'is_floating'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-11efb4a72e29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetworkLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0mflat_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         logging.vlog(\n\u001b[1;32m    986\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The dtype of the target tensor must be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.dtype' object has no attribute 'is_floating'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the discounted rewards for the entire episode and normalize it\n",
    "discounted_rewards = discountRewards(rewards,  discountfactor=self.discountfactor)\n",
    "if self.normalizeRewards:\n",
    "    discounted_rewards = (discounted_rewards - np.mean(discounted_rewards))/ (np.std(discounted_rewards) + 1e-7)            # to avoid division by 0\n",
    "\n",
    "\n",
    "#TODO -----> add eager execution\n",
    "losses = []\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    for curStates, actions, nextStates, rewards, done, actionProb in self.memory:\n",
    "\n",
    "        # get log prob\n",
    "        logProb = np.log(actionProb)\n",
    "        sampleloss = logProb * discounted_rewards\n",
    "\n",
    "        losses.append(-sampleloss)      # this is negative, since we are interested in gradient ascent\n",
    "\n",
    "    networkLoss = np.sum(losses)\n",
    "\n",
    "grads = tape.gradient(networkLoss, self.PolicyNetwork.model.trainable_variables)\n",
    "self.PolicyNetwork.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for all experience in batchsize\n",
    "curStates       = np.vstack(list(list(zip(*Agent.memory)))[0])\n",
    "actions         = np.vstack(list(list(zip(*Agent.memory)))[1])\n",
    "nextStates      = np.vstack(list(list(zip(*Agent.memory)))[3])\n",
    "rewards         = np.vstack(list(list(zip(*Agent.memory)))[2])\n",
    "done            = np.vstack(list(list(zip(*Agent.memory)))[4])\n",
    "\n",
    "actionProb      = np.vstack(list(list(zip(*Agent.memory)))[5])\n",
    "\n",
    "\n",
    "# Get X\n",
    "try:\n",
    "    Agent.env.observation_space.n\n",
    "    X_train = getOneHotrepresentation(curStates, num_classes=Agent.inputShape)\n",
    "except:\n",
    "    X_train = curStates    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.03113103, -0.02077829,  0.00398832,  0.00614775],\n",
       "        [-0.03154659,  0.17428624,  0.00411127, -0.28527415],\n",
       "        [-0.02806087, -0.0208941 , -0.00159421,  0.0087026 ],\n",
       "        [-0.02847875,  0.17425067, -0.00142016, -0.28448289]]),\n",
       " array([[0.5012765 , 0.49872348],\n",
       "        [0.5122483 , 0.48775175],\n",
       "        [0.50089973, 0.49910033],\n",
       "        [0.51175475, 0.4882453 ]], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:4], actionProb[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([14.85422289]),\n",
       " array([13.99416454]),\n",
       " array([13.12541872]),\n",
       " array([12.2478977])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_rewards = discountRewards(rewards,  discountfactor=Agent.discountfactor)\n",
    "discounted_rewards[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.58898873],\n",
       "       [1.38696682],\n",
       "       [1.1829043 ],\n",
       "       [0.97678053]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Agent.baseline.upper() == \"VALUE\":\n",
    "    value = Agent.ValueNetwork.model.predict(X_train)\n",
    "    G = discounted_rewards - value\n",
    "\n",
    "\n",
    "elif Agent.baseline.upper() == \"NORMALIZE\":\n",
    "    G = discounted_rewards\n",
    "    G = (G - np.mean(G))/ (np.std(G) + 1e-7)            # to avoid division by 0\n",
    "\n",
    "G[:4]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5012765 ,  0.5012765 ],\n",
       "       [ 0.48775172, -0.48775175],\n",
       "       [-0.50089973,  0.5008997 ],\n",
       "       [ 0.48824525, -0.4882453 ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = np.subtract(RLUtils.getOneHotrepresentation(actions,Agent.env.action_space.n ), actionProb)\n",
    "gradient[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7965227 ,  0.7965227 ],\n",
       "       [ 0.67649543, -0.6764955 ],\n",
       "       [-0.5925164 ,  0.59251636],\n",
       "       [ 0.47690845, -0.4769085 ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient *= G \n",
    "gradient[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.9652265e-05,  7.9652265e-05],\n",
       "       [ 6.7649540e-05, -6.7649547e-05],\n",
       "       [-5.9251641e-05,  5.9251634e-05],\n",
       "       [ 4.7690843e-05, -4.7690850e-05]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient *= Agent.policy_learning_rate\n",
    "gradient[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old model\n",
    "old = tf.keras.models.clone_model(Agent.PolicyNetwork.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# updating actual probabilities (y_train) to take into account the change in policy gradient change\n",
    "# \\theta = \\theta + alpha*rewards * gradient\n",
    "y_train = actionProb + np.vstack(gradient)\n",
    "\n",
    "\n",
    "\n",
    "# update policy and also learn the value function\n",
    "\n",
    "# 1. Update policy\n",
    "history = Agent.PolicyNetwork.model.train_on_batch(X_train, y_train)\n",
    "\n",
    "Agent.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49950963, 0.50049037],\n",
       "       [0.5345103 , 0.46548966],\n",
       "       [0.49908236, 0.5009177 ],\n",
       "       [0.53361577, 0.4663842 ]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old.predict(X_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5013335 , 0.49866652],\n",
       "       [0.5123426 , 0.4876574 ],\n",
       "       [0.5009569 , 0.4990431 ],\n",
       "       [0.51184905, 0.48815095]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.PolicyNetwork.model.predict(X_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41494c8c96ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_probability'"
     ]
    }
   ],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(porto)",
   "language": "python",
   "name": "porto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
