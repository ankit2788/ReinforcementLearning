{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from importlib import reload\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "# get the relative path\n",
    "pref                = os.getcwd()\n",
    "\n",
    "os.environ[\"RL_PATH\"]   = pref\n",
    "\n",
    "#os.environ[\"RL_PATH\"] = \"/Users/ankitgupta/Documents/git/anks/Books/ReinforcementLearning/DeepQLearning\"\n",
    "#pref = os.environ[\"RL_PATH\"]\n",
    "\n",
    "if f'{pref}/RLLibrary' not in sys.path:\n",
    "    sys.path.append(f'{pref}/RLLibrary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger PG_Train (INFO)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# importing custom libraries\n",
    "from ActionSelection import ActionExploration\n",
    "from ConfigReader import Config\n",
    "#from RLAgents import QLearningAgent, FittedQAgent, DQN, DoubleDQN\n",
    "import PolicyGradientAgents\n",
    "import utils as RLUtils\n",
    "from Agents.PolicyGradient import Reinforce, ActorCritic\n",
    "from Agents.PolicyGradient import discountRewards\n",
    "import loggingConfig as loggingConfig\n",
    "\n",
    "\n",
    "logger = loggingConfig.logging\n",
    "logger.getLogger(\"PG_Train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Agents.PolicyGradient.Reinforce' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/Agents/PolicyGradient/Reinforce.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile  = os.path.join(pref, \"Configs.ini\" )\n",
    "savePath    = os.path.join(os.environ[\"RL_PATH\"], \"models\" )\n",
    "_time       = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParams = {\"Name\": \"Policy\", \"NetworkShape\": [1024, 512], \"learning_rate\": 0.01, \\\n",
    "                \"optimizer\": \"ADAM\", \"loss\": \"categorical_crossentropy\", }\n",
    "valueParams = {\"Name_value\": \"Value\", \"NetworkShape_value\": [24, 12], \"learning_rate_value\": 0.0001, \\\n",
    "                \"optimizer_value\": \"ADAM\", \"loss_value\": \"mse\", }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Agents.PolicyGradient.ActorCritic' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/Agents/PolicyGradient/ActorCritic.py'>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ActorCritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReinforceAgent   = ActorCritic.ActorCritic(env = env, configFile = configFile, **modelParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x64cdf7ac8>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReinforceAgent.SharedNetwork.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SharedDesign\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1024)         5120        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 512)          524800      dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 2)            1026        dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1)            513         dense_33[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 531,459\n",
      "Trainable params: 531,459\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ReinforceAgent.SharedNetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets run for 1 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.04276836, -0.01055716,  0.03850685, -0.00280508]),\n",
       " 0,\n",
       " 1.0,\n",
       " array([ 0.04255722, -0.20620959,  0.03845075,  0.30177423]),\n",
       " False,\n",
       " array([0.49938238, 0.5006177 ], dtype=float32),\n",
       " -0.0016929482)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.02299997, -0.1845979 , -0.00340317,  0.26377976]),\n",
       "  0,\n",
       "  1.0,\n",
       "  array([ 0.01930802, -0.37967111,  0.00187243,  0.55538736]),\n",
       "  False,\n",
       "  array([0.5164557 , 0.48354435], dtype=float32),\n",
       "  8.142222)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.996466, shape=(), dtype=float32) tf.Tensor(8.142222, shape=(), dtype=float32) tf.Tensor(0.0545969, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.5164557  0.48354435]], shape=(1, 2), dtype=float32)\n",
      "action rob for action:  0 Prob:  0.5164557\n",
      "losses 0.0029808215 0.036075763\n",
      "[<tf.Tensor: id=12728, shape=(), dtype=float32, numpy=0.6921267>] []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'dense_26/kernel:0', 'dense_26/bias:0', 'dense_27/kernel:0', 'dense_27/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-28be36445259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetworkLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSharedNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSharedNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSharedNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# reset memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m   1023\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m-> 1025\u001b[0;31m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[1;32m   1026\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     logging.warning(\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'dense_26/kernel:0', 'dense_26/bias:0', 'dense_27/kernel:0', 'dense_27/bias:0']."
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "\n",
    "    \n",
    "    for sample in Agent.memory:\n",
    "        \n",
    "        # get log prob\n",
    "        state   = tf.Variable([sample[0]], trainable = True,   dtype=tf.float32)\n",
    "        nextstate   = tf.Variable([sample[3]], trainable = True,   dtype=tf.float32)\n",
    "        action, reward, dead  = sample[1], sample[2], sample[4]\n",
    "        \n",
    "\n",
    "        #nextState = tf.convert_to_tensor([nextState], dtype=tf.float32)\n",
    "\n",
    "        # run shared network to get action prob distro and value associated with that state\n",
    "        actionProbDistro, _currStateValue = Agent.SharedNetwork(state, training = True)\n",
    "        _actionProbDistro, _nextStateValue = Agent.SharedNetwork(nextState, training = True)\n",
    "        \n",
    "        #print(actionProbDistro, _currStateValue, _actionProbDistro, _nextStateValue)\n",
    "\n",
    "        # compute TD error\n",
    "        # delta = r + gamma*V_(t+1) - V_t\n",
    "        delta = reward + Agent.discountfactor* tf.squeeze(_nextStateValue) * (1 - int(dead)) - tf.squeeze(_currStateValue)\n",
    "        print(tf.squeeze(_nextStateValue), tf.squeeze(_currStateValue), delta)\n",
    "\n",
    "        # compute critic loss\n",
    "        loss_sample_critic = delta**2\n",
    "\n",
    "        # compute actor loss\n",
    "        print(actionProbDistro)\n",
    "        \n",
    "        actionProb      = actionProbDistro.numpy()[0, action]\n",
    "        print(\"action rob for action: \", action, \"Prob: \", actionProb)\n",
    "        loss_sample_actor =  tf.math.log(actionProb) * delta\n",
    "        print(\"losses\", loss_sample_critic.numpy(), -loss_sample_actor.numpy())\n",
    "        \n",
    "                \n",
    "        print(loss_actor, loss_critic)\n",
    "\n",
    "\n",
    "    networkLoss = sum(loss_critic) + sum(loss_actor)\n",
    "\n",
    "    logger.info(f\"{Agent.Name} - Updating Policy \")\n",
    "\n",
    "    # performing Backpropagation to update the network\n",
    "    \n",
    "    grads = tape.gradient(networkLoss, Agent.SharedNetwork.trainable_variables)\n",
    "    print(grads)\n",
    "    Agent.SharedNetwork.optimizer.apply_gradients(zip(grads, Agent.SharedNetwork.trainable_variables))\n",
    "\n",
    "    # reset memory\n",
    "    #Agent.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=2740, shape=(4,), dtype=float32, numpy=array([ 0.04276836, -0.01055716,  0.03850685, -0.00280508], dtype=float32)>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03763838, -0.03630947, -0.01659229, -0.03680008])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03763838, -0.03630947, -0.01659229, -0.03680008]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_currentState = _currentState.reshape([1, _currentState.shape[0]])\n",
    "_currentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "act, val = Agent.SharedNetwork(_currentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1686, shape=(1, 2), dtype=float32, numpy=array([[0.49874383, 0.5012562 ]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49874383, 0.5012562 ], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "act /= tf.reduce_sum(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0007066454"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1131, shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action             = np.random.choice(Agent.env.action_space.n, p = act.numpy()[0])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-61fb5aae8d2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for all experience in batchsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcurStates\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mactions\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnextStates\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrewards\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# for all experience in batchsize\n",
    "curStates       = np.vstack(list(list(zip(*Agent.memory)))[0])\n",
    "actions         = np.vstack(list(list(zip(*Agent.memory)))[1])\n",
    "nextStates      = np.vstack(list(list(zip(*Agent.memory)))[3])\n",
    "rewards         = np.vstack(list(list(zip(*Agent.memory)))[2])\n",
    "done            = np.vstack(list(list(zip(*Agent.memory)))[4])\n",
    "\n",
    "actionProb      = np.vstack(list(list(zip(*Agent.memory)))[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the discounted rewards for the entire episode and normalize it\n",
    "discounted_rewards = discountRewards(rewards,  discountfactor=Agent.discountfactor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounted_rewards = (discounted_rewards - np.mean(discounted_rewards))/ (np.std(discounted_rewards) + 1e-7)            # to avoid division by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905667496168143"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_rewards[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    for index, sample in enumerate(Agent.memory):\n",
    "\n",
    "        # get log prob\n",
    "        state = tf.Variable([sample[0]], trainable=True, dtype=tf.float64)\n",
    "        prob = Agent.PolicyNetwork.model(state, training = True)\n",
    "        action = sample[1]\n",
    "        actionProb = prob[0, action]\n",
    "        logProb = tf.math.log(actionProb)\n",
    "\n",
    "        #actionProb = sample[5]\n",
    "        #logProb = np.log(actionProb)\n",
    "        sampleloss = logProb * discounted_rewards[index][0]\n",
    "\n",
    "        losses.append(-sampleloss)      # this is negative, since we are interested in gradient ascent\n",
    "\n",
    "    networkLoss = sum(losses)\n",
    "    \n",
    "    grads = tape.gradient(networkLoss, Agent.PolicyNetwork.model.trainable_variables)\n",
    "    Agent.PolicyNetwork.optimizer.apply_gradients(zip(grads, Agent.PolicyNetwork.model.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.12274747, -1.90964119,  0.19205285,  2.89212446]),\n",
       " 1,\n",
       " 1.0,\n",
       " array([-0.16094029, -1.7161645 ,  0.24989534,  2.66336226]),\n",
       " True,\n",
       " array([0.7354947 , 0.26450533], dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5343, shape=(1, 2), dtype=float32, numpy=array([[0.7354947, 0.2645053]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5352, shape=(), dtype=float32, numpy=0.2645053>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = sample[1]\n",
    "actionProb = prob[0, action]\n",
    "actionProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5344, shape=(1, 2), dtype=float32, numpy=array([[-0.307212, -1.329894]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.dtype' object has no attribute 'is_floating'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-11efb4a72e29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetworkLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0mflat_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         logging.vlog(\n\u001b[1;32m    986\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The dtype of the target tensor must be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.dtype' object has no attribute 'is_floating'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the discounted rewards for the entire episode and normalize it\n",
    "discounted_rewards = discountRewards(rewards,  discountfactor=self.discountfactor)\n",
    "if self.normalizeRewards:\n",
    "    discounted_rewards = (discounted_rewards - np.mean(discounted_rewards))/ (np.std(discounted_rewards) + 1e-7)            # to avoid division by 0\n",
    "\n",
    "\n",
    "#TODO -----> add eager execution\n",
    "losses = []\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    for curStates, actions, nextStates, rewards, done, actionProb in self.memory:\n",
    "\n",
    "        # get log prob\n",
    "        logProb = np.log(actionProb)\n",
    "        sampleloss = logProb * discounted_rewards\n",
    "\n",
    "        losses.append(-sampleloss)      # this is negative, since we are interested in gradient ascent\n",
    "\n",
    "    networkLoss = np.sum(losses)\n",
    "\n",
    "grads = tape.gradient(networkLoss, self.PolicyNetwork.model.trainable_variables)\n",
    "self.PolicyNetwork.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for all experience in batchsize\n",
    "curStates       = np.vstack(list(list(zip(*Agent.memory)))[0])\n",
    "actions         = np.vstack(list(list(zip(*Agent.memory)))[1])\n",
    "nextStates      = np.vstack(list(list(zip(*Agent.memory)))[3])\n",
    "rewards         = np.vstack(list(list(zip(*Agent.memory)))[2])\n",
    "done            = np.vstack(list(list(zip(*Agent.memory)))[4])\n",
    "\n",
    "actionProb      = np.vstack(list(list(zip(*Agent.memory)))[5])\n",
    "\n",
    "\n",
    "# Get X\n",
    "try:\n",
    "    Agent.env.observation_space.n\n",
    "    X_train = getOneHotrepresentation(curStates, num_classes=Agent.inputShape)\n",
    "except:\n",
    "    X_train = curStates    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.03113103, -0.02077829,  0.00398832,  0.00614775],\n",
       "        [-0.03154659,  0.17428624,  0.00411127, -0.28527415],\n",
       "        [-0.02806087, -0.0208941 , -0.00159421,  0.0087026 ],\n",
       "        [-0.02847875,  0.17425067, -0.00142016, -0.28448289]]),\n",
       " array([[0.5012765 , 0.49872348],\n",
       "        [0.5122483 , 0.48775175],\n",
       "        [0.50089973, 0.49910033],\n",
       "        [0.51175475, 0.4882453 ]], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:4], actionProb[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([14.85422289]),\n",
       " array([13.99416454]),\n",
       " array([13.12541872]),\n",
       " array([12.2478977])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_rewards = discountRewards(rewards,  discountfactor=Agent.discountfactor)\n",
    "discounted_rewards[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.58898873],\n",
       "       [1.38696682],\n",
       "       [1.1829043 ],\n",
       "       [0.97678053]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Agent.baseline.upper() == \"VALUE\":\n",
    "    value = Agent.ValueNetwork.model.predict(X_train)\n",
    "    G = discounted_rewards - value\n",
    "\n",
    "\n",
    "elif Agent.baseline.upper() == \"NORMALIZE\":\n",
    "    G = discounted_rewards\n",
    "    G = (G - np.mean(G))/ (np.std(G) + 1e-7)            # to avoid division by 0\n",
    "\n",
    "G[:4]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5012765 ,  0.5012765 ],\n",
       "       [ 0.48775172, -0.48775175],\n",
       "       [-0.50089973,  0.5008997 ],\n",
       "       [ 0.48824525, -0.4882453 ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = np.subtract(RLUtils.getOneHotrepresentation(actions,Agent.env.action_space.n ), actionProb)\n",
    "gradient[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7965227 ,  0.7965227 ],\n",
       "       [ 0.67649543, -0.6764955 ],\n",
       "       [-0.5925164 ,  0.59251636],\n",
       "       [ 0.47690845, -0.4769085 ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient *= G \n",
    "gradient[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.9652265e-05,  7.9652265e-05],\n",
       "       [ 6.7649540e-05, -6.7649547e-05],\n",
       "       [-5.9251641e-05,  5.9251634e-05],\n",
       "       [ 4.7690843e-05, -4.7690850e-05]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient *= Agent.policy_learning_rate\n",
    "gradient[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old model\n",
    "old = tf.keras.models.clone_model(Agent.PolicyNetwork.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# updating actual probabilities (y_train) to take into account the change in policy gradient change\n",
    "# \\theta = \\theta + alpha*rewards * gradient\n",
    "y_train = actionProb + np.vstack(gradient)\n",
    "\n",
    "\n",
    "\n",
    "# update policy and also learn the value function\n",
    "\n",
    "# 1. Update policy\n",
    "history = Agent.PolicyNetwork.model.train_on_batch(X_train, y_train)\n",
    "\n",
    "Agent.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49950963, 0.50049037],\n",
       "       [0.5345103 , 0.46548966],\n",
       "       [0.49908236, 0.5009177 ],\n",
       "       [0.53361577, 0.4663842 ]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old.predict(X_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5013335 , 0.49866652],\n",
       "       [0.5123426 , 0.4876574 ],\n",
       "       [0.5009569 , 0.4990431 ],\n",
       "       [0.51184905, 0.48815095]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent.PolicyNetwork.model.predict(X_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41494c8c96ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_probability'"
     ]
    }
   ],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1, shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [[0. ,1.],\n",
    " [0. ,1.],\n",
    " [1. ,0.],\n",
    " [0. ,1.],\n",
    " [0. ,1.]]\n",
    "\n",
    "action = tf.convert_to_tensor(actions, dtype = tf.float32)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3, shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0.49889222, 0.5011078 ],\n",
       "       [0.5098192 , 0.4901808 ],\n",
       "       [0.49936554, 0.5006344 ],\n",
       "       [0.51149035, 0.48850965],\n",
       "       [0.524573  , 0.4754269 ]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [0,0,1,1,1]\n",
    "\n",
    "prob = [[0.49889222, 0.5011078 ],\n",
    " [0.5098192 , 0.4901808 ],\n",
    " [0.49936554, 0.5006344 ],\n",
    " [0.51149035, 0.48850965],\n",
    " [0.524573   ,0.4754269 ]]\n",
    "probs = tf.convert_to_tensor(prob, dtype = tf.float32)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5011078 ],\n",
       "       [0.        , 0.4901808 ],\n",
       "       [0.49936554, 0.        ],\n",
       "       [0.        , 0.48850965],\n",
       "       [0.        , 0.4754269 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(actions)*np.array(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49889222, 0.5098192, 0.5006344, 0.48850965, 0.4754269]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_action = []\n",
    "for index, _prob in enumerate(probs.numpy()):\n",
    "    prob_action.append(_prob[actions[index]])\n",
    "    \n",
    "prob_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4, shape=(5, 1), dtype=float32, numpy=\n",
       "array([[0.49889222],\n",
       "       [0.5098192 ],\n",
       "       [0.5006344 ],\n",
       "       [0.48850965],\n",
       "       [0.4754269 ]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_action = tf.convert_to_tensor(np.array(prob_action).reshape(-1,1), dtype = tf.float32)\n",
    "prob_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=5, shape=(5, 1), dtype=float32, numpy=\n",
       " array([[4.8847766],\n",
       "        [3.933328 ],\n",
       "        [2.9502857],\n",
       "        [1.9861633],\n",
       "        [0.9786838]], dtype=float32)>,\n",
       " TensorShape([5, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages = [[4.8847766],\n",
    " [3.933328 ],\n",
    " [2.9502857],\n",
    " [1.9861633],\n",
    " [0.9786838]]\n",
    "\n",
    "advantages = tf.convert_to_tensor(advantages, dtype = tf.float32)\n",
    "advantages, advantages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=24, shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-0.6953652 ],\n",
       "       [-0.6736991 ],\n",
       "       [-0.6918792 ],\n",
       "       [-0.71639603],\n",
       "       [-0.74354213]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.log(prob_action + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=26, shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-4.8847766],\n",
       "       [-3.933328 ],\n",
       "       [-2.9502857],\n",
       "       [-1.9861633],\n",
       "       [-0.9786838]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-tf.stop_gradient(advantages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shaep after (5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=50, shape=(1,), dtype=float32, numpy=array([10.238397], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"shaep after\", prob_action.shape)\n",
    "loss_actor = tf.reduce_sum(-tf.stop_gradient(advantages) * tf.math.log(prob_action + 1e-10), axis=0)\n",
    "\n",
    "loss_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.238396760000002"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [3.3967037 , 2.6498795 , 2.0412414 , 1.4228795 , 0.72769266]\n",
    "sum(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=66, shape=(1,), dtype=float32, numpy=array([1.740222], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entropy loss\n",
    "loss_entropy = tf.reduce_sum(-1 * prob_action * tf.math.log(prob_action + 1e-10), axis=0)\n",
    "\n",
    "loss_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=90, shape=(), dtype=float32, numpy=22.56641>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total loss\n",
    "total_loss = tf.reduce_mean(loss_actor + loss_entropy + loss_critic)\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=70, shape=(), dtype=float32, numpy=10.587792>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_critic = tf.reduce_mean(tf.math.square(advantages))\n",
    "loss_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=80, shape=(1,), dtype=float32, numpy=array([22.56641], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss = loss_actor + loss_entropy + loss_critic\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------tf.Tensor(20.853111, shape=(), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # forward run the netwrok to get probabilities and value functions for all states in memory \n",
    "        probs, values = self.localModel(tf.convert_to_tensor(np.vstack(memory.states), dtype = tf.float32))\n",
    "\n",
    "        advantages = discounted_rewards - values.numpy()\n",
    "        advantages = tf.convert_to_tensor(advantages, dtype = tf.float32)\n",
    "\n",
    "        # critic loss\n",
    "        loss_critic = tf.reduce_mean(tf.math.square(advantages))\n",
    "\n",
    "        # actor loss\n",
    "        onehotActions = tf.one_hot(memory.actions, self.action_size, dtype =tf.float32)\n",
    "\n",
    "        # prob of action taken\n",
    "        prob_action = []\n",
    "        for index, _prob in enumerate(probs.numpy()):\n",
    "            prob_action.append(_prob[memory.actions[index]])\n",
    "\n",
    "        prob_action = tf.convert_to_tensor(np.array(prob_action).reshape(-1,1), dtype = tf.float32)\n",
    "        loss_actor = tf.reduce_sum(-tf.stop_gradient(advantages) * tf.math.log(prob_action + 1e-10), axis=0)\n",
    "\n",
    "        # entropy loss\n",
    "        loss_entropy = tf.reduce_sum(-1 * prob_action * tf.math.log(prob_action + 1e-10), axis=0)\n",
    "\n",
    "        # total loss\n",
    "        total_loss = tf.reduce_mean(loss_actor + beta_entropy*loss_entropy + loss_critic)\n",
    "        print(total_loss)\n",
    "        return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss critic tf.Tensor(\n",
      "[[23.861042 ]\n",
      " [15.471068 ]\n",
      " [ 8.7041855]\n",
      " [ 3.9448445]\n",
      " [ 0.957822 ]], shape=(5, 1), dtype=float32)\n",
      "action loss tf.Tensor(\n",
      "[[3.3967037 ]\n",
      " [2.6498795 ]\n",
      " [2.0412414 ]\n",
      " [1.4228795 ]\n",
      " [0.72769266]], shape=(5, 1), dtype=float32)\n",
      "entropy loss tf.Tensor(\n",
      "[[0.3469123 ]\n",
      " [0.34346473]\n",
      " [0.3463785 ]\n",
      " [0.34996638]\n",
      " [0.35349995]], shape=(5, 1), dtype=float32)\n",
      "mean loss tf.Tensor(12.983516, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# critic loss\n",
    "loss_critic = tf.math.square(advantages)\n",
    "print(\"loss critic\",loss_critic)\n",
    "# prob of action taken\n",
    "prob_action = []\n",
    "for index, _prob in enumerate(probs.numpy()):\n",
    "    prob_action.append(_prob[actions[index]])\n",
    "\n",
    "prob_action = tf.convert_to_tensor(np.array(prob_action).reshape(-1,1), dtype = tf.float32)\n",
    "\n",
    "loss_actor = -tf.stop_gradient(advantages) * tf.math.log(prob_action + 1e-10)\n",
    "print(\"action loss\",loss_actor)\n",
    "\n",
    "# entropy loss\n",
    "loss_entropy = -1 * prob_action * tf.math.log(prob_action + 1e-10)\n",
    "print(\"entropy loss\",loss_entropy)\n",
    "\n",
    "# total loss\n",
    "total_loss = tf.reduce_mean(loss_actor + loss_entropy + loss_critic)\n",
    "print(\"mean loss\",total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=160, shape=(5, 1), dtype=float32, numpy=\n",
       "array([[27.604658 ],\n",
       "       [18.464413 ],\n",
       "       [11.091805 ],\n",
       "       [ 5.7176905],\n",
       "       [ 2.0390146]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_actor + loss_entropy + loss_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agents.PolicyGradient.A3C import Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Agents.PolicyGradient.A3C.Agents' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/Agents/PolicyGradient/A3C/Agents.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir         = f\"{pref}/models/Policy/A3C\"\n",
    "optimizer_args = {\"learning_rate\": 1e-4}\n",
    "model = Agents.MasterAgent(game = \"CartPole-v0\", save_dir=save_dir, **optimizer_args)\n",
    "\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "import multiprocessing\n",
    "reward_queue = Queue()\n",
    "\n",
    "global_episode_index = multiprocessing.Value(\"i\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = Agents.WorkerAgent(state_size = model.state_size, action_size = model.action_size, \\\n",
    "                    global_model = model.globalModel, sharedOptimizer = model.optimizer, \\\n",
    "                    result_queue = reward_queue, global_episode_index = global_episode_index, \\\n",
    "                    workerIndex = 1, gameName = model.gameName , save_dir = model.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Agents.PolicyGradient.A3C.Models.ActorCritic at 0x646c2b6d8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local.localModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(local):\n",
    "\n",
    "    # runs the Agent forever (until all games played)\n",
    "    # trains the agent while sharing the network parameters \n",
    "    local.memory = Agents.Memory()\n",
    "    total_steps = 1\n",
    "\n",
    "    while local.worker_episode_index.value < 2000:\n",
    "\n",
    "        # run the agent for each episode\n",
    "\n",
    "        current_state = local.env.reset()\n",
    "        local.memory.reset()\n",
    "\n",
    "        episodic_reward = 0\n",
    "        episodic_steps= 0\n",
    "        local.episodicLoss = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            probs, _ = local.localModel(tf.convert_to_tensor(current_state[None, :], dtype = tf.float32))\n",
    "            #print(\"current_state after\", tf.convert_to_tensor(current_state[None, :]))\n",
    "            action = np.random.choice(local.action_size, p = probs.numpy()[0])  # since probs is a tensor. need to convert to array\n",
    "\n",
    "            # take a step towards the action\n",
    "            next_state, reward, done, _ = local.env.step(action)\n",
    "\n",
    "            if done:\n",
    "                reward = -1\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "            # store the transition in memory\n",
    "            local.memory.update(current_state, action, reward)\n",
    "\n",
    "\n",
    "            if total_steps%5 == 0 or done:\n",
    "                # --- this is where the local model is being trained and global model gets udpated\n",
    "\n",
    "                # track the variables involved in loss computation using GradientTape (eager execution)\n",
    "                with tf.GradientTape() as tape:\n",
    "\n",
    "                    # get the reward value at final step\n",
    "                    if done:\n",
    "                        reward_sum = 0\n",
    "                    else:\n",
    "                        _, values = local.localModel(tf.convert_to_tensor(next_state[None, :], dtype=tf.float32))\n",
    "                        reward_sum = values.numpy()[0]\n",
    "\n",
    "                    # Here we are assuming 1 step return\n",
    "\n",
    "                    # get discounted returns\n",
    "                    discounted_rewards = []\n",
    "                    discount_factor = 0.99\n",
    "\n",
    "                    for reward in local.memory.rewards[::-1]:\n",
    "\n",
    "                        reward_sum = reward + discount_factor*reward_sum\n",
    "                        discounted_rewards.append(reward_sum)\n",
    "\n",
    "                    discounted_rewards.reverse()\n",
    "                    discounted_rewards = np.array(discounted_rewards).reshape(-1,1)\n",
    "\n",
    "\n",
    "                    # forward run the netwrok to get probabilities and value functions for all states in memory \n",
    "                    probs, values = local.localModel(tf.convert_to_tensor(np.vstack(local.memory.states), dtype = tf.float32))\n",
    "\n",
    "                    advantages = discounted_rewards - values\n",
    "                    advantages = tf.convert_to_tensor(advantages, dtype = tf.float32)\n",
    "\n",
    "\n",
    "                    # critic loss\n",
    "                    loss_critic = tf.math.square(advantages)\n",
    "\n",
    "                    # actor loss\n",
    "                    \n",
    "                    actions= [[index, item] for index, item in enumerate(local.memory.actions)]\n",
    "                    prob_action = tf.convert_to_tensor(tf.gather_nd(probs, actions)[:, None])\n",
    "                    #prob_action = tf.gather_nd(probs, actions)\n",
    "                    print(\"actionprob\", prob_action.shape)\n",
    "                    \n",
    "                    # prob of action taken\n",
    "\n",
    "                    loss_actor_new = tf.math.log(prob_action + 1e-10)\n",
    "                    print(\"loss_actor\", loss_actor_new)\n",
    "\n",
    "                    loss_actor_new *= - tf.stop_gradient(advantages)\n",
    "                    print(\"loss_actor\", loss_actor_new)\n",
    "\n",
    "                #     loss_actor = tf.reduce_sum(-tf.stop_gradient(advantages) * tf.math.log(probs + 1e-10), axis=1)\n",
    "\n",
    "                    # entropy loss\n",
    "                    loss_entropy = tf.math.log(prob_action + 1e-10)\n",
    "                    loss_entropy *= - prob_action\n",
    "\n",
    "                    #loss_entropy = tf.reduce_sum(-1 * probs * tf.math.log(probs + 1e-10), axis=1)\n",
    "\n",
    "                    # total loss\n",
    "                    print(\"total loss\", loss_critic + loss_actor_new)\n",
    "                    total_loss = tf.reduce_mean(loss_entropy)\n",
    "                    print(\"total loss mean\", total_loss)\n",
    "                    #total_loss = tf.reduce_mean(loss_actor_new + beta_entropy*loss_entropy + loss_critic)\n",
    "                    \n",
    "\n",
    "                    #     loss_actor = tf.reduce_sum(-tf.stop_gradient(advantages) * tf.math.log(probs + 1e-10), axis=1)\n",
    "\n",
    "                        # entropy loss\n",
    "                    #     loss_entropy = tf.math.log(prob_action + 1e-10)\n",
    "                    #     loss_entropy *= - prob_action\n",
    "\n",
    "                        #loss_entropy = tf.reduce_sum(-1 * probs * tf.math.log(probs + 1e-10), axis=1)\n",
    "                    #print([var.name for var in tape.watched_variables()])\n",
    "                local.episodicLoss += total_loss\n",
    "\n",
    "                # compute the gradients with respect to local model\n",
    "                grads = tape.gradient(total_loss, local.localModel.trainable_weights)\n",
    "                for grad in grads:\n",
    "                    if grad is None:\n",
    "                        print(\"None grad\")\n",
    "                    else:\n",
    "                        print(grad.shape)\n",
    "                \n",
    "                #print(total_loss)\n",
    "                #print(grads)\n",
    "\n",
    "                sys.exit(0)\n",
    "\n",
    "\n",
    "            total_steps += 1\n",
    "            current_state = next_state\n",
    "\n",
    "            episodic_steps += 1                 # we are not using it\n",
    "\n",
    "\n",
    "        logger.info(f'{self.Name} Episode: {self.worker_episode_index.value}  Reward: {episodic_reward}.   Best Score (GLOBAL): {WorkerAgent.best_score}  ')\n",
    "\n",
    "        with local.worker_episode_index.get_lock():\n",
    "            # update the global episode numner\n",
    "            local.result_queue.put(episodic_reward)\n",
    "            local.worker_episode_index.value += 1\n",
    "\n",
    "\n",
    "    # once the agent has exhausted all runs\n",
    "    local.result_queue.put(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actionprob (5, 1)\n",
      "loss_actor tf.Tensor(\n",
      "[[-0.68606836]\n",
      " [-0.7007931 ]\n",
      " [-0.68661636]\n",
      " [-0.68586445]\n",
      " [-0.6801466 ]], shape=(5, 1), dtype=float32)\n",
      "loss_actor tf.Tensor(\n",
      "[[3.389187 ]\n",
      " [2.7817233]\n",
      " [2.0702212]\n",
      " [1.3867172]\n",
      " [0.6913328]], shape=(5, 1), dtype=float32)\n",
      "total loss tf.Tensor(\n",
      "[[27.792925]\n",
      " [18.537806]\n",
      " [11.161086]\n",
      " [ 5.474608]\n",
      " [ 1.724497]], shape=(5, 1), dtype=float32)\n",
      "total loss mean tf.Tensor(0.3457437, shape=(), dtype=float32)\n",
      "(4, 128)\n",
      "(128,)\n",
      "(128, 2)\n",
      "(2,)\n",
      "None grad\n",
      "None grad\n",
      "None grad\n",
      "None grad\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "run(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, values = local.localModel(tf.convert_to_tensor(np.vstack(local.memory.states), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7751, shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0.5020227 , 0.49797735],\n",
       "       [0.50882125, 0.49117878],\n",
       "       [0.51637334, 0.48362663],\n",
       "       [0.5243915 , 0.47560853],\n",
       "       [0.5181277 , 0.4818724 ]], dtype=float32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local.memory.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [1, 1], [2, 1], [3, 1], [4, 0]]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions= [[index, item] for index, item in enumerate(local.memory.actions)]\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9958, shape=(5,), dtype=float32, numpy=\n",
       "array([0.49797735, 0.49117878, 0.48362663, 0.47560853, 0.5181277 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather_nd(probs, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.convert_to_tensor(tf.gather_nd(probs, actions)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 1])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9956, shape=(5, 1), dtype=float32, numpy=\n",
       "array([[0.49797735],\n",
       "       [0.49117878],\n",
       "       [0.48362663],\n",
       "       [0.47560853],\n",
       "       [0.5181277 ]], dtype=float32)>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(2, (1,3), activation='relu', input_shape=(11, 50,3)))\n",
    "model.add(layers.Conv2D(1, (1,48), activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 11, 48, 2)         20        \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 11, 1, 1)          97        \n",
      "=================================================================\n",
      "Total params: 117\n",
      "Trainable params: 117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(layers.RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(units = 20, input_shape=(11, 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20)                5680      \n",
      "=================================================================\n",
      "Total params: 5,680\n",
      "Trainable params: 5,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----LSTM training\n",
    "import os\n",
    "\n",
    "LIB_PATH = os.environ[\"RL_PATH\"]\n",
    "LOG_PATH = f'{LIB_PATH}/../logs/'\n",
    "DATA_PATH = \"/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/UseCases/Port_2Stock/data\"\n",
    "\n",
    "_path = os.path.join(DATA_PATH, f\"APA.csv\")\n",
    "_thisData = pd.read_csv(_path, index_col = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-12-31</th>\n",
       "      <td>32.783871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02</th>\n",
       "      <td>32.500908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-05</th>\n",
       "      <td>32.969822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-06</th>\n",
       "      <td>33.115353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-07</th>\n",
       "      <td>32.945576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close\n",
       "Date                 \n",
       "2003-12-31  32.783871\n",
       "2004-01-02  32.500908\n",
       "2004-01-05  32.969822\n",
       "2004-01-06  33.115353\n",
       "2004-01-07  32.945576"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_thisData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = mm.fit_transform(_thisData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(60, 3000):\n",
    "    x_train.append(training_set[i-60:i,0])\n",
    "    y_train.append(training_set[i,0])\n",
    "    \n",
    "  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2940, 60, 1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(x_train)\n",
    "Y_train = np.array(y_train)\n",
    "X_train.shape\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 20)                1760      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,781\n",
      "Trainable params: 1,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(units = 20,  input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.Dropout(.2))\n",
    "\n",
    "\n",
    "#model.add(layers.LSTM(units = 50))\n",
    "#model.add(layers.Dropout(.2))\n",
    "\"\"\"\n",
    "model.add(layers.LSTM(units = 50, return_sequences=True))\n",
    "model.add(layers.Dropout(.2))\n",
    "\"\"\"\n",
    "model.add(layers.Dense(units = 1))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2940 samples\n",
      "2940/2940 [==============================] - 15s 5ms/sample - loss: 0.0584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a547235c0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29918742]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = X_train[100]\n",
    "model.predict(test.reshape((1,60,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2707841246583246"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new model\n",
    "new_model = models.Sequential()\n",
    "new_model.add(layers.LSTM(units = 20,  input_shape=(X_train.shape[1], 1)))\n",
    "#new_model.add(layers.Dropout(.2))\n",
    "#set weights of the first layer\n",
    "new_model.set_weights(model.layers[0].get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08292898, -0.11602986,  0.0293291 ,  0.06014059,  0.07082775,\n",
       "        -0.09266429, -0.12714677,  0.13329615,  0.10601458,  0.1527101 ,\n",
       "        -0.05965486, -0.04044554,  0.17166282,  0.01756974, -0.01239254,\n",
       "        -0.07096303,  0.14257011, -0.04356703,  0.19897966,  0.04398943]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(test.reshape((1,60,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08292898, -0.11602986,  0.0293291 ,  0.06014059,  0.07082775,\n",
       "        -0.09266429, -0.12714677,  0.13329615,  0.10601458,  0.1527101 ,\n",
       "        -0.05965486, -0.04044554,  0.17166282,  0.01756974, -0.01239254,\n",
       "        -0.07096303,  0.14257011, -0.04356703,  0.19897966,  0.04398943]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new model\n",
    "new_model = models.Sequential()\n",
    "new_model.add(layers.LSTM(units = 20,  input_shape=(X_train.shape[1], 1)))\n",
    "new_model.add(layers.Dropout(.2))\n",
    "#set weights of the first layer\n",
    "new_model.set_weights(model.layers[0].get_weights())\n",
    "new_model.predict(test.reshape((1,60,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(porto)",
   "language": "python",
   "name": "porto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
