{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "# get the relative path\n",
    "pref                = os.getcwd()\n",
    "\n",
    "os.environ[\"RL_PATH\"]   = pref\n",
    "\n",
    "if f'{pref}/RLLibrary' not in sys.path:\n",
    "    sys.path.append(f'{pref}/RLLibrary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLLibrary.utils import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([EnvSpec(PortfolioManagement-v0), EnvSpec(PortfolioManagement_CNN-v0), EnvSpec(OrderExecution-v0)])\n"
     ]
    }
   ],
   "source": [
    "from RLLibrary.FinUseCases.OrderExecution import EnvironmentManager\n",
    "from RLLibrary.FinUseCases.OrderExecution import ExecutionAlgos\n",
    "from RLLibrary.utils import constants as constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([EnvSpec(PortfolioManagement-v0), EnvSpec(PortfolioManagement_CNN-v0), EnvSpec(OrderExecution-v0)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'RLLibrary.FinUseCases.OrderExecution.EnvironmentManager' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/FinUseCases/OrderExecution/EnvironmentManager.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ExecutionAlgos)\n",
    "reload(EnvironmentManager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = constants.DATA_DIR\n",
    "MODEL_DIR = constants.MODEL_DIR\n",
    "\n",
    "path = os.path.join(MODEL_DIR, \"OrderExecution\")    \n",
    "path = os.path.join(path, \"DQN\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "path = os.path.join(path, \"FF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"ticker\": \"RELIANCE\", \"trainingYear\": [\"2018\" , \"2019\"], \\\n",
    "            \"penalizeFactors\": {\"Impact\": -100, \"StepReward\": 0.1}}\n",
    "\n",
    "stgy = ExecutionAlgos.Execution_DQN(envName = \"OrderExecution-v0\", **args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLLibrary.FinUseCases.OrderExecution.ModelManager.DQN import Agent as DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RLLibrary.FinUseCases.OrderExecution.ModelManager.DQN.Agent' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/FinUseCases/OrderExecution/ModelManager/DQN/Agent.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(DQNAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for 2018\n",
      "Loading Data for 2019\n",
      "Model: \"nn_ff\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  693       \n",
      "=================================================================\n",
      "Total params: 1,877\n",
      "Trainable params: 1,813\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"nn_ff_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  693       \n",
      "=================================================================\n",
      "Total params: 1,877\n",
      "Trainable params: 1,813\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(MODEL_DIR, \"OrderExecution\")\n",
    "MAX_EPISODES = 1\n",
    "DQNModel = None\n",
    "hiddenUnits = [32]\n",
    "batchNormalization = True\n",
    "dropout_rate = 0.25\n",
    "optimizer_learning_rate = 1e-4\n",
    "clipvalue = 100\n",
    "\n",
    "\n",
    "networkArgs = {\"Model\": DQNModel, \"hiddenUnits\": hiddenUnits, \"batchNormalization\": batchNormalization, \\\n",
    "                \"dropout_rate\" : dropout_rate, \"optimizer_learning_rate\": optimizer_learning_rate, \"clipvalue\": clipvalue}\n",
    "\n",
    "\n",
    "stgy.agent = DQNAgent.DQN(envName = stgy.envName, save_dir = save_dir, \\\n",
    "                            networkArgs = networkArgs,  **stgy.envargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLLibrary.FinUseCases.OrderExecution.ModelManager.DQN.Agent import epsilon_exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Episode\", ep)\n",
    "ep_reward = 0\n",
    "ep_steps = 0\n",
    "done = False\n",
    "\n",
    "stgy.agent.env.reset()\n",
    "\n",
    "current_state = stgy.agent.env.observation_space.currentState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.17"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 10\n",
    "new_state, reward, done, _ = stgy.agent.env.step(action)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0342556966984768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(action*stgy.agent.env.orderSizeFactor/stgy.agent.env.currentInfo.Volume)**2. * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action*stgy.agent.env.orderSizeFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "WARNING:tensorflow:Layer nn_ff_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "WARNING:tensorflow:Layer nn_ff_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Action 0\n",
      "Executing remaining order 20.0\n",
      "Action 20.0\n",
      "Executing remaining order 20.0 index 20.0 remaining 0.0 True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eps         = epsilon_exploration(nbFrames=0)\n",
    "totalFrames = 0\n",
    "discount_factor = 0.99\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "episodic_rewards = []\n",
    "\n",
    "grads = None\n",
    "MAX_EPISODES = 1\n",
    "for ep in range(MAX_EPISODES):\n",
    "\n",
    "    print(\"Episode\", ep)\n",
    "    ep_reward = 0\n",
    "    ep_steps = 0\n",
    "    done = False\n",
    "\n",
    "    stgy.agent.env.reset()\n",
    "\n",
    "    current_state = stgy.agent.env.observation_space.currentState\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # get action\n",
    "        current_state = np.array(current_state).reshape(1, stgy.agent.env.observation_space.n)\n",
    "\n",
    "\n",
    "        trainingRequired = False\n",
    "        laststep = False\n",
    "        if ep_steps % stgy.agent.env.TimeGapbetweenIntervals == 0:\n",
    "            # take model defined action \n",
    "            actionIndex = stgy.agent.getAction(current_state, eps, mode = \"TRAIN\"    )\n",
    "            action = stgy.agent.env.action_space.actions[actionIndex]\n",
    "            \n",
    "            actionIndex = 0\n",
    "            action = 0\n",
    "            trainingRequired = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if stgy.agent.env.currentInfo.TimeLeft == 1:\n",
    "            # if only 1 minute left, execute all remaining orders\n",
    "\n",
    "            action = stgy.agent.env.currentInfo.AvailableInventory/ stgy.agent.env.orderSizeFactor\n",
    "            actionIndex = action            # in this case, action and actionIndex are same\n",
    "            trainingRequired =True\n",
    "            print(\"Executing remaining order\", action)\n",
    "            laststep = True\n",
    "\n",
    "\n",
    "        if trainingRequired:\n",
    "            new_state, reward, done, _ = stgy.agent.env.step(action)\n",
    "            if laststep:\n",
    "                print(\"Executing remaining order\", action, \"index\", actionIndex, \"remaining\", stgy.agent.env.currentInfo.AvailableInventory, done)\n",
    "\n",
    "\n",
    "            # store into experience replay\n",
    "            stgy.agent.updateMemory(current_state, actionIndex, reward, new_state, done)\n",
    "\n",
    "            # train teh model\n",
    "            grads = stgy.agent.learn(done, episodeCount = ep, discount_factor = discount_factor, batch_size = batch_size)\n",
    "            ep_reward += reward\n",
    "            # Logger.info(f'Ep#: {ep} Reward:{reward} Action: {action} Steps: {ep_steps} leftSize: {self.env.currentInfo.AvailableInventory}    penality factor: {self.env.RewardManager.impactPenalizeFactor} {action} {self.env.orderSizeFactor} '   )\n",
    "\n",
    "        else:\n",
    "            action = 0\n",
    "            new_state, _ , done, _ = stgy.agent.env.step(action)\n",
    "\n",
    "\n",
    "        # update state\n",
    "        ep_steps += 1\n",
    "        current_state = new_state\n",
    "\n",
    "\n",
    "    finishTime = time.perf_counter()\n",
    "\n",
    "    # update Logs\n",
    "\n",
    "    # get final portfolio value\n",
    "#     orderHistory = stgy.agent.env.getInventoryHistory()\n",
    "#     inventory = orderHistory[\"AvailableInventory\"].iloc[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = random.sample(stgy.agent.memory.memory, batch_size)\n",
    "\n",
    "\n",
    "# for all experience in batchsize\n",
    "curStates       = np.array([tup[0][0] for tup in samples])\n",
    "actions         = np.array([int(tup[1]) for tup in samples])\n",
    "nextStates      = np.array([tup[2] for tup in samples])\n",
    "rewards         = np.array([tup[3] for tup in samples])\n",
    "done            = np.array([tup[4] for tup in samples])\n",
    "\n",
    "# Add pre-processing step if needed\n",
    "\n",
    "inputStates     = np.array(curStates).reshape(len(curStates), stgy.agent.env.observation_space.n)\n",
    "nextStates      = np.array(nextStates).reshape(len(nextStates), stgy.agent.env.observation_space.n)\n",
    "\n",
    "# ------ For DQN, target comes from the target model\n",
    "# using bellman equation \n",
    "\n",
    "# Steps:\n",
    "# 1. use target model to set the target. \n",
    "# 2. This target needs to be updated based on the bellman equation\n",
    "# 2.1. Bellman Equation 1: get the max Q values for next state in the bellman equation\n",
    "\n",
    "\n",
    "curr_q_vaues    = stgy.agent.train_model(inputStates) \n",
    "new_q_values    = stgy.agent.target_model(nextStates) \n",
    "\n",
    "# -----DQN algo ------\n",
    "# vectorized computation\n",
    "\n",
    "target          = rewards + discount_factor * np.amax(new_q_values, axis=1)\n",
    "target[done]    = rewards[done]            # end state target is reward itself\n",
    "\n",
    "target_f        = curr_q_vaues.numpy().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 21)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3022187 ,  0.18379895,  0.11962121, -0.05420659, -0.18451208,\n",
       "        0.1926317 , -0.23073354,  0.30779052, -0.01637991, -0.04174974,\n",
       "        0.2126154 , -0.52619237,  0.36619782,  0.17128167,  0.24842678,\n",
       "        0.39681095, -0.07346999,  0.32926083,  0.80459636,  0.27718922,\n",
       "        0.10671678,  0.23389345, -0.04841739, -0.11149358, -0.0315684 ,\n",
       "        0.03399301,  0.17515258, -0.04098229,  0.07166208,  0.02889245,\n",
       "        0.10766726,  0.4003131 ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_f[range(batch_size), actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1bd20dee12bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "target_f[range(batch_size), actions] = target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>OpenInventory</th>\n",
       "      <th>TimeLeft</th>\n",
       "      <th>ExecutedInventory</th>\n",
       "      <th>ExecutedPrice</th>\n",
       "      <th>AvailableInventory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09:30</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09:31</td>\n",
       "      <td>10000</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1206.75</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09:32</td>\n",
       "      <td>10000</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1207.15</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09:33</td>\n",
       "      <td>10000</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1207.95</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09:34</td>\n",
       "      <td>10000</td>\n",
       "      <td>96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1209.40</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>11:06</td>\n",
       "      <td>10000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1207.55</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>11:07</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>11:08</td>\n",
       "      <td>10000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1208.95</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>11:09</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1208.40</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>11:10</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1207.55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time  OpenInventory  TimeLeft  ExecutedInventory  ExecutedPrice  \\\n",
       "0    09:30          10000       100                0.0            NaN   \n",
       "1    09:31          10000        99                0.0        1206.75   \n",
       "2    09:32          10000        98                0.0        1207.15   \n",
       "3    09:33          10000        97                0.0        1207.95   \n",
       "4    09:34          10000        96                0.0        1209.40   \n",
       "..     ...            ...       ...                ...            ...   \n",
       "96   11:06          10000         4                0.0        1207.55   \n",
       "97   11:07          10000         3                0.0        1208.00   \n",
       "98   11:08          10000         2                0.0        1208.95   \n",
       "99   11:09          10000         1                0.0        1208.40   \n",
       "100  11:10          10000         0            10000.0        1207.55   \n",
       "\n",
       "     AvailableInventory  \n",
       "0               10000.0  \n",
       "1               10000.0  \n",
       "2               10000.0  \n",
       "3               10000.0  \n",
       "4               10000.0  \n",
       "..                  ...  \n",
       "96              10000.0  \n",
       "97              10000.0  \n",
       "98              10000.0  \n",
       "99              10000.0  \n",
       "100                 0.0  \n",
       "\n",
       "[101 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stgy.agent.env.getInventoryHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1153.15, 1153.35, 1152.65, 1152.95])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for 2018\n",
      "Loading Data for 2019\n",
      "Loading Data for 2020\n"
     ]
    }
   ],
   "source": [
    "a.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.getVolumeStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50302.118086978924"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.VolumeStats.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = plt.hist(a.VolumeStats.Data, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RLLibrary.FinUseCases.OrderExecution.EnvironmentManager' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/FinUseCases/OrderExecution/EnvironmentManager.py'>"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(EnvironmentManager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for 2018\n",
      "998.25\n"
     ]
    }
   ],
   "source": [
    "a = EnvironmentManager.OrderExecution(ticker = \"RELIANCE\", trainingYear=[\"2018\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>OpenInventory</th>\n",
       "      <th>TimeLeft</th>\n",
       "      <th>ExecutedInventory</th>\n",
       "      <th>ExecutedPrice</th>\n",
       "      <th>AvailableInventory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09:30</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time  OpenInventory  TimeLeft  ExecutedInventory  ExecutedPrice  \\\n",
       "0  09:30          10000       100                  0            NaN   \n",
       "\n",
       "   AvailableInventory  \n",
       "0               10000  "
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.getInventoryHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[998.25, 998.95, 996.15, 998.  ],\n",
       "        [997.55, 998.  , 995.15, 995.45],\n",
       "        [995.6 , 997.55, 995.35, 997.55],\n",
       "        [997.7 , 998.  , 997.55, 997.65]]),\n",
       " array([[1.        , 1.00070123, 0.99789632, 0.99974956],\n",
       "        [0.99929877, 0.99974956, 0.99689457, 0.99719509],\n",
       "        [0.99734535, 0.99929877, 0.99709492, 0.99929877],\n",
       "        [0.99944904, 0.99974956, 0.99929877, 0.99939895]]))"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.envData[1][:4], a.envData[1][:4]/998.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874.0565095846646"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "875 * 0.9989217252396166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, '09:30', 1244.0)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.currentInfo.Index, a.currentInfo.Time, a.envData[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "newState, reward, over, _ = a.step(action = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-525.0"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>OpenInventory</th>\n",
       "      <th>TimeLeft</th>\n",
       "      <th>ExecutedInventory</th>\n",
       "      <th>ExecutedPrice</th>\n",
       "      <th>AvailableInventory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09:30</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09:31</td>\n",
       "      <td>10000</td>\n",
       "      <td>99</td>\n",
       "      <td>500</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>9500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time  OpenInventory  TimeLeft  ExecutedInventory  ExecutedPrice  \\\n",
       "0  09:30          10000       100                  0            NaN   \n",
       "1  09:31          10000        99                500         1263.0   \n",
       "\n",
       "   AvailableInventory  \n",
       "0               10000  \n",
       "1                9500  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.getInventoryHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from RLLibrary.FinUseCases import CustomGym\n",
    "from RLLibrary.utils import constants as constants\n",
    "DATA_DIR = constants.DATA_DIR\n",
    "MODEL_DIR = constants.MODEL_DIR\n",
    "\n",
    "# ------register PortfolioManagement environment onto Custom gym\n",
    "\n",
    "CustomGym.register(\n",
    "    id = \"OrderExecution-v0\",\n",
    "    entry_point = 'FinUseCases.OrderExecution.EnvironmentManager:OrderExecution',\n",
    "    kwargs = {\"ticker\": \"RELIANCE\", \\\n",
    "                \"orderConfig\": {\"initialOrderSize\": 10000, \"initialTimeHorizon\": 100, \"orderFactor\": 100, \\\n",
    "                                \"TotalIntervals\": 50, \"startTime\": \"09:30\", \"Timezone\": \"IST\"}, \\\n",
    "\n",
    "                \"nbHistory\" : 15, \\\n",
    "                \"trainingYear\" : [], \"testDate\" : None, \\\n",
    "                \"dataPath\" : os.path.join(DATA_DIR, \"OrderExecution\"), \\\n",
    "                'penalizeFactors' : {\"Impact\": -0.01}})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLLibrary.FinUseCases.OrderExecution.ModelManager.DQN import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RLLibrary.FinUseCases.OrderExecution.ModelManager.DQN.Agent' from '/Users/ankitgupta/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/FinUseCases/OrderExecution/ModelManager/DQN/Agent.py'>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for 2018\n",
      "Loading Data for 2019\n",
      "Model: \"nn_ff_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  3333      \n",
      "=================================================================\n",
      "Total params: 4,517\n",
      "Trainable params: 4,453\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"nn_ff_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch multiple                  128       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  3333      \n",
      "=================================================================\n",
      "Total params: 4,517\n",
      "Trainable params: 4,453\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_args = {\"ticker\": \"RELIANCE\", \"trainingYear\": [\"2018\" , \"2019\"]}\n",
    "envName = \"OrderExecution-v0\"\n",
    "save_dir = MODEL_DIR\n",
    "agent = Agent.DQN(envName, save_dir,  \\\n",
    "                    networkArgs  = {\"Model\": None, \"hiddenUnits\": [32], \\\n",
    "                        \"batchNormalization\": True, \"dropout_rate\" : 0.25, \"optimizer_learning_rate\": 1e-4, \"clipvalue\": 100},\n",
    "                        **env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer nn_ff_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer nn_ff_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [01:26<2:59:06, 21.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-387-f0cd77f8b953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/FinUseCases/OrderExecution/ModelManager/DQN/Agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, MAX_EPISODES, discount_factor, batch_size)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;31m# train teh model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodeCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# update state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/anks/MachineLearning/ReinforcementLearning/RLLibrary/FinUseCases/OrderExecution/ModelManager/DQN/Agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, terminal_state, episodeCount, discount_factor, batch_size)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_AddGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m     \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_input_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   1573\u001b[0m       gen_math_ops._sum(\n\u001b[1;32m   1574\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m           name=name))\n\u001b[0m\u001b[1;32m   1576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/porto/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  11146\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11147\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keep_dims\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11148\u001b[0;31m         keep_dims)\n\u001b[0m\u001b[1;32m  11149\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11150\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(MAX_EPISODES = 500, discount_factor = 0.99, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.25000000000001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-6*(500*7)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(porto)",
   "language": "python",
   "name": "porto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
